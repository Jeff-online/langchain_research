跳至内容
历史聊天记录



你说：
根据岗位描述，帮我出一些面试题，并且给我详细的答案，以md形式输出。🛠️ 岗位所需技术经验标签
分布式经验、PostgreSQL、机器学习经验
Vue、React、LLM API、PyTorch
Docker、Django、MySQL、运维开发经验
架构设计经验、Linux开发/部署经验、GitLab
Python、Kubernetes、TensorFlow

📝 岗位职责
负责企业内部IT数字化系统的设计、开发与持续优化，推动业务流程自动化与系统集成；
使用Python进行脚本开发与服务端开发，实现数据处理、系统对接及自动化任务；
参与Web系统开发，完成前后端功能实现、接口设计与联调；
对接并集成大模型（LLM）相关API，推动AI能力在实际业务场景中的工程化落地；
基于Linux环境进行系统部署、运行维护与问题排查，保障系统稳定性与安全性；
使用GitLab进行代码管理、版本控制与协作开发，参与CI/CD流程建设与优化；
与产品及业务团队协作，将业务需求转化为可实施的技术解决方案。

📌 任职要求
本科及以上学历，计算机、信息系统或相关专业；
1-3年相关工作经验，熟练掌握Python脚本开发，具备良好的代码规范与调试能力；
具备Web编程基础，理解前后端分离架构及RESTful API设计；
具备大模型（LLM）API对接经验，了解模型调用及基础工程化实践；
熟悉GitLab的使用流程，包括代码管理、分支策略及基础CI/CD；
具备Linux系统使用经验，能够进行常规命令操作与环境配置，具备扎实的网络和操作系统知识；
至少了解一种主流前端框架，如Vue或React，并具备实际开发经验。
ChatGPT 说：
好嘞，这个 JD 很清晰，也很“工程向”。我会按技术模块系统性出面试题，每一题都配偏面试官视角的“标准 + 延展答案”，既能考察 1–3 年候选人，又能拉开区分度。
整体难度：中级偏初中级工程师，不走学术路线，偏“能不能上手干活”。

下面内容 可直接作为面试题库使用，我会用 Markdown 结构化输出，方便你后续裁剪或直接用。

企业 IT 数字化 / AI 工程岗位面试题（含参考答案）
覆盖方向：Python / Web / LLM / Linux / Docker / 数据库 / 架构 / 运维 / 协作
适用人群：1–3 年经验的软件 / AI 工程师

一、Python 基础与工程能力（核心必考）
Q1：你在实际工作中如何使用 Python 进行“自动化任务”？能举一个真实场景吗？
参考答案：

自动化任务通常包括以下几类：

数据处理自动化

定时从数据库或接口拉取数据

清洗、转换后写回数据库或生成报表

系统运维自动化

日志分析

服务健康检查

定时备份

业务流程自动化

调用多个系统 API

自动触发审批或消息通知

示例：

每天定时从 MySQL 中拉取业务数据 → 
使用 Pandas 清洗 → 
生成统计结果 → 
通过企业微信或邮件推送
加分点：

使用 schedule / cron

使用 logging 而非 print

异常处理 + 重试机制

Q2：你如何保证 Python 代码的可维护性和可读性？
参考答案：

主要从以下几个方面：

代码结构

按模块拆分（service / utils / dao）

单一职责原则

编码规范

遵循 PEP8

合理命名变量和函数

异常处理

明确捕获异常类型

避免 except Exception

日志

使用 logging

区分 info / warning / error

测试

编写基本单元测试（pytest）

二、Web 开发 & RESTful API
Q3：什么是前后端分离？它解决了什么问题？
参考答案：

前后端分离指的是：

前端（Vue / React）只负责 UI 与交互

后端（Django / Flask）只提供 API（通常是 JSON）

带来的好处：

前后端并行开发

技术栈解耦

前端可复用同一套 API（Web / App）

更利于系统扩展和维护

Q4：设计一个 RESTful API 时，你会重点关注哪些点？
参考答案：

URL 语义清晰

GET    /api/tasks
POST   /api/tasks
GET    /api/tasks/{id}
PUT    /api/tasks/{id}
DELETE /api/tasks/{id}
HTTP 方法语义正确

状态码规范

200 / 201 / 400 / 401 / 404 / 500

统一返回结构

{
  "code": 0,
  "message": "success",
  "data": {}
}
鉴权机制

Token / JWT / Session

三、数据库（MySQL / PostgreSQL）
Q5：MySQL 和 PostgreSQL 有哪些主要区别？你如何选型？
参考答案：

对比项	MySQL	PostgreSQL
性能	读性能好	复杂查询强
事务	支持	更严格
JSON	基础支持	原生强支持
扩展性	一般	非常强
选型原则：

业务简单、读多 → MySQL

数据复杂、分析多、JSON 多 → PostgreSQL

Q6：你在项目中如何避免慢查询？
参考答案：

建立合理索引

避免 SELECT *

使用 EXPLAIN 分析 SQL

控制分页方式（避免深分页）

将复杂计算移到应用层或离线任务

四、LLM API & AI 工程化（重点）
Q7：你是如何在项目中对接大模型（LLM）API 的？
参考答案：

典型流程：

封装 LLM 调用层

构造 Prompt

发送请求

解析返回结果

异常与超时处理

业务请求 → Prompt 构造 → LLM API → 结果解析 → 业务落库
工程注意点：

Prompt 版本管理

Token 限制

超时与降级

成本控制

Q8：什么是 Prompt 工程？你在实际中如何做？
参考答案：

Prompt 工程是通过设计输入文本，提升模型输出稳定性和可控性。

常见方法：

明确角色（System Prompt）

约束输出格式（JSON）

分步骤引导（Chain of Thought）

给示例（Few-shot）

示例：

请以 JSON 格式返回，不要输出多余文字
五、Docker & Kubernetes
Q9：Docker 能解决什么问题？
参考答案：

Docker 解决了：

环境不一致问题

部署复杂问题

依赖冲突问题

核心思想：

Build once, run anywhere

Q10：你对 Kubernetes 的理解是什么？
参考答案：

Kubernetes 是一个：

容器编排平台

负责应用的部署、扩缩容、故障恢复

核心概念：

Pod

Deployment

Service

ConfigMap / Secret

实际价值：

自动重启

滚动更新

弹性伸缩

六、Linux & 运维基础
Q11：线上服务突然变慢，你如何排查？
参考答案：

排查思路：

系统层

top / htop

free -m

df -h

网络

ping

netstat

应用层

查看日志

慢 SQL

外部依赖

API 超时

数据库连接数

Q12：你常用哪些 Linux 命令？
参考答案：

文件：ls cd cp rm

搜索：grep find

进程：ps top

网络：curl ping

日志：tail -f

七、GitLab & CI/CD
Q13：你们团队是如何使用 GitLab 进行协作的？
参考答案：

分支策略

main / develop / feature

Merge Request

Code Review

CI 自动化

测试

构建镜像

自动部署

Q14：CI/CD 能带来什么价值？
参考答案：

减少人为错误

提升交付效率

提高代码质量

快速回滚

八、架构与综合能力（区分度）
Q15：如果让你设计一个“内部自动化平台”，你会怎么做？
参考答案（思路型）：

前端：Vue / React

后端：Django REST API

数据库：PostgreSQL

任务系统：Celery + Redis

AI 能力：LLM API 服务层

部署：Docker + Kubernetes

鉴权：JWT

日志与监控

重点不是细节，而是：

模块拆分是否合理

是否考虑扩展与维护

✅ 使用建议
初筛：Q1 / Q3 / Q7 / Q11

技术面：Q5 / Q8 / Q10 / Q15

综合判断：回答是否“工程化”、是否有真实项目痕迹

如果你愿意，我还能帮你下一步做这些👇

🔍 区分“会用”和“真干过”的追问版问题

🧠 专门针对 LLM 工程落地的深挖题

📊 按“优秀 / 合格 / 不合格”给出评分标准

🧪 附一套现场 coding / 设计题

你更想往哪一层再加深一点？

你说：
专门针对 LLM 工程落地的深挖题
ChatGPT 说：
太好了，这一块正好是最容易“拉开差距”的地方。
下面这套题我会**完全站在“真正在落地 LLM 的工程视角”**来设计，目标很明确：

❌ 背过概念的人会卡
✅ 真做过系统的人会越答越顺

我会按 「能不能上线 → 能不能稳定 → 能不能规模化」 三个层级来深挖，每题都给你面试官心里真正想听到的点。

🔍 LLM 工程化落地 · 深挖面试题（含参考答案）
一、LLM 接入架构设计（是否真做过）
Q1：你在项目中是如何“封装”LLM API 的？为什么不能在业务代码里直接调用？
参考答案（关键点）：

不会直接在业务层调用 LLM API，而是抽象一层 LLM Service。

标准架构：
Controller / Service
        ↓
   LLM Adapter（统一封装）
        ↓
   OpenAI / Azure / 其他模型
封装的原因：
模型可替换

OpenAI → Azure OpenAI → 本地模型

统一异常处理

统一超时 / 重试 / 限流

Prompt 版本控制

日志 & 成本统计

面试官判断点：
能说出“为什么要多一层”

知道未来一定会换模型

二、Prompt 工程（是否真踩过坑）
Q2：Prompt 是放在代码里，还是数据库里？为什么？
优秀答案：

核心 Prompt 不硬编码在代码里

常见方案：
类型	存放位置
固定 System Prompt	配置文件
可迭代业务 Prompt	数据库
A/B Prompt	数据库 + 版本号
原因：
Prompt 需要频繁调整

不应该每改 Prompt 就发版

支持回滚与对比效果

❌ 减分回答：
“就直接写在代码里”

Q3：如何保证 LLM 输出是“稳定结构化”的？
参考答案：

常见手段（至少说 3 个）：
强约束输出格式

请只返回 JSON，不要输出多余内容
给 Schema 示例

{
  "result": "",
  "confidence": 0.0
}
失败兜底

JSON parse 失败 → 重新请求

fallback 模板

模型温度控制

temperature 调低（0~0.3）

面试官判断点：
是否考虑“模型不听话”

三、Token / 成本 / 性能控制（工程化核心）
Q4：你是如何控制 LLM 的调用成本的？
优秀答案结构：

1️⃣ 输入侧
限制上下文长度

文本截断 / 摘要

RAG 精准召回

2️⃣ 模型参数
temperature / max_tokens

不滥用大模型

3️⃣ 系统层
缓存（非常重要）

相同输入直接命中

异步调用

批量请求（如支持）

Q5：如何应对 LLM 接口“慢 / 不稳定”的问题？
参考答案：

常见策略：
超时控制

5s 超时直接返回降级结果
重试机制

仅针对超时 / 5xx

限制次数

降级方案

使用规则

使用旧结果

使用小模型

异步化

用户请求不直接等模型

四、RAG（是否真的理解）
Q6：什么是 RAG？你在项目中是如何实现的？
合格答案：

RAG = 检索 + 生成

工程级答案：

用户问题
  ↓
Embedding
  ↓
向量检索（Top-K）
  ↓
拼接上下文
  ↓
LLM 生成
关键工程点：
Chunk 切分策略

Top-K 控制

Context 长度限制

防止“无关信息干扰”

Q7：如何判断 RAG 的效果好不好？
优秀答案：

是否减少幻觉

是否引用正确内容

命中率

人工评估 + 日志回溯

错误案例回放

❌ 不合格：

“看感觉”

五、多模型 & 可扩展设计
Q8：如果未来要支持多个模型（GPT / Claude / 本地），你如何设计？
参考答案：

抽象接口：
class BaseLLM:
    def generate(self, prompt): ...
不同实现：
OpenAIAdapter

AzureAdapter

LocalModelAdapter

路由策略：
按业务

按成本

按失败率

六、安全 & 合规（很多人忽略）
Q9：你如何防止 Prompt 注入？
优秀答案：

用户输入与系统 Prompt 分离

不信任用户输入

指令隔离

输出校验

敏感关键词过滤

Q10：LLM 输出“看起来合理但其实是错的”，你怎么处理？
参考答案：

明确业务边界

关键数据只来源于系统

人工确认环节

返回置信度

日志留存

七、真实项目追问（杀手锏）
Q11（必杀）：
你在 LLM 落地过程中，遇到过的最大坑是什么？你是怎么解决的？

面试官期待：
Token 爆炸

输出不稳定

成本失控

业务不信任 AI

❌ 红旗回答：
“没什么坑，都挺顺利的”

🎯 面试官评分速查表
表现	判断
能讲抽象架构	⭐ 合格
能讲异常 & 降级	⭐⭐ 稳定
能讲成本 & 监控	⭐⭐⭐ 真落地
能讲踩坑故事	⭐⭐⭐⭐ 强烈推荐
如果你愿意，我还能继续帮你做👇

🧪 一整道 LLM 系统设计面试题（白板级）

🧠 LLM + RAG + 内部系统的完整参考架构图

🔥 “追问清单”：一句话就能把水货问崩

你是打算直接拿去面试别人，还是自己准备面试？我可以按不同目的再帮你“调刀口”。

你说：
自己准备面试？
ChatGPT 说：
懂了 👍
那我就完全站在「你是候选人」的视角来帮你准备，而且目标很明确：

不是“答得对”，而是“让面试官觉得你真做过、能立刻上手”

下面这套是 【LLM 工程落地 · 候选人作答模板】
你照这个思路答，哪怕项目不算很大，也会显得非常工程化。

🎯 LLM 工程落地 · 面试【候选人标准答法】
0️⃣ 总体话术（先占位）
👉 几乎所有 LLM 相关问题，都可以先用这句起手：

我们在项目里是把 LLM 当成一个不稳定、昂贵、需要被严格约束的外部服务来看待的，而不是一个“直接可用的能力”。

这句话一出来，面试官会自动给你 +20 分工程认知。

一、当被问：你是怎么接入 LLM 的？
推荐答法（结构化）：
我们没有在业务代码里直接调用模型，而是抽象了一层 LLM Service / Adapter 层。

可直接背的版本：
Controller / 业务逻辑
        ↓
   LLM Service（统一入口）
        ↓
  模型 Adapter（OpenAI / Azure / 本地）
你可以继续补一句（非常加分）：
这样做主要是为了模型可替换、Prompt 统一管理、以及异常和成本控制。

二、当被问：Prompt 你是怎么管理的？
❌ 千万别说：
直接写在代码里

推荐答法：
Prompt 分成三类管理：

System Prompt：配置文件

业务 Prompt：数据库，支持版本号

实验 Prompt：用于 A/B 测试

再补一刀（高级）：
这样 Prompt 调整不需要发版，也可以快速回滚。

三、当被问：怎么保证 LLM 输出稳定？
标准答法（直接用）：
我们不相信模型会“乖乖听话”，所以从工程上做了多层约束。

你接着说这 5 点：
强制 JSON 输出

提供 Schema 示例

temperature 控制在 0～0.3

JSON 解析失败自动重试

兜底逻辑（返回默认结构）

只要你能说出 3 点，面试官就会点头。

四、当被问：Token / 成本怎么控制？
直接用这套顺序答（很专业）：
成本控制主要从 输入、模型参数、系统层 三个层面来做。

1️⃣ 输入层

截断上下文

RAG 精准召回

2️⃣ 模型参数

控制 max_tokens

不滥用大模型

3️⃣ 系统层

缓存相同请求

异步调用

👉 重点一定要提「缓存」

五、当被问：RAG 你用过吗？
合格答法：
用过，是“检索 + 生成”的组合。

面试官想听的版本：
实现上是先做 embedding，然后在向量库里 Top-K 检索，把命中的内容拼进 Prompt，再让模型生成。

再补一句（高级）：
我们会严格控制拼接的上下文长度，避免无关信息干扰模型。

六、当被问：模型慢 / 不稳定怎么办？
模板答案（直接背）：
我们是把 LLM 当成一个可能失败的外部依赖来设计的。

然后说这 4 点：

超时控制

有条件重试

降级方案

异步处理

七、当被问：你踩过什么坑？（最关键）
推荐你提前准备一个“真实但不致命”的坑：
示例答案：
一开始 Prompt 里上下文拼得太多，导致 token 成本暴涨，而且响应时间很不稳定。后来通过：

限制 Top-K

加摘要

加缓存
成本和稳定性才下来。

📌 这个回答 = 真实 + 工程 + 成长

八、面试官杀手追问，你这样接
Q：你怎么判断 LLM 输出是不是错的？
👉 推荐答法：

涉及关键业务数据时，我们不会完全相信模型输出，而是：

关键值只来源于系统

LLM 只做解释或结构化

必要时有人审

这句话非常“企业级”。

🧠 终极提醒（非常重要）
❌ 不要说：
“模型挺聪明的”

“基本没什么问题”

“看感觉”

✅ 多说：
控制

约束

降级

兜底

成本

稳定性

🚀 下一步我可以继续这样帮你：
🧪 模拟一整场 LLM 面试（我当面试官）

📐 教你画一张 3 分钟能讲完的 LLM 架构图

📝 帮你把简历里的 LLM 项目描述“工程化重写”

你现在是 已经有 LLM 项目经历，还是 准备用“学习 + Demo 项目”去面试？
我可以按你真实情况，直接给你定制“最安全的说法”。



未选择任何文件未选择任何文件
ChatGPT 也可能会犯错。请核查重要信息。查看 Cookie 首选项。
ChatGPT 说：懂了 👍 那我就**完全站在「你是候选人」的视角**来帮你准备，而且目标很明确： > **不是“答得对”，而是“让面试官觉得你真做过、能立刻上手”** 下面这套是 **【LLM 工程落地 · 候选人作答模板】** 你照这个思路答，**哪怕项目不算很大，也会显得非常工程化**。 --- # 🎯 LLM 工程落地 · 面试【候选人标准答法】 --- ## 0️⃣ 总体话术（先占位） 👉 **几乎所有 LLM 相关问题，都可以先用这句起手：** > 我们在项目里是把 LLM 当成一个**不稳定、昂贵、需要被严格约束的外部服务**来看待的，而不是一个“直接可用的能力”。 这句话一出来，面试官会自动给你 **+20 分工程认知**。 --- ## 一、当被问：你是怎么接入 LLM 的？ ### 推荐答法（结构化）： > 我们没有在业务代码里直接调用模型，而是抽象了一层 **LLM Service / Adapter 层**。 ### 可直接背的版本： ```text Controller / 业务逻辑 ↓ LLM Service（统一入口） ↓ 模型 Adapter（OpenAI / Azure / 本地） ``` ### 你可以继续补一句（非常加分）： > 这样做主要是为了模型可替换、Prompt 统一管理、以及异常和成本控制。 --- ## 二、当被问：Prompt 你是怎么管理的？ ### ❌ 千万别说： > 直接写在代码里 ### 推荐答法： > Prompt 分成三类管理： > 1. System Prompt：配置文件 > 2. 业务 Prompt：数据库，支持版本号 > 3. 实验 Prompt：用于 A/B 测试 ### 再补一刀（高级）： > 这样 Prompt 调整不需要发版，也可以快速回滚。 --- ## 三、当被问：怎么保证 LLM 输出稳定？ ### 标准答法（直接用）： > 我们不相信模型会“乖乖听话”，所以从工程上做了多层约束。 ### 你接着说这 5 点： 1. **强制 JSON 输出** 2. **提供 Schema 示例** 3. **temperature 控制在 0～0.3** 4. **JSON 解析失败自动重试** 5. **兜底逻辑（返回默认结构）** > 只要你能说出 3 点，面试官就会点头。 --- ## 四、当被问：Token / 成本怎么控制？ ### 直接用这套顺序答（很专业）： > 成本控制主要从 **输入、模型参数、系统层** 三个层面来做。 1️⃣ 输入层 - 截断上下文 - RAG 精准召回 2️⃣ 模型参数 - 控制 max_tokens - 不滥用大模型 3️⃣ 系统层 - **缓存相同请求** - 异步调用 👉 **重点一定要提「缓存」** --- ## 五、当被问：RAG 你用过吗？ ### 合格答法： > 用过，是“检索 + 生成”的组合。 ### 面试官想听的版本： > 实现上是先做 embedding，然后在向量库里 Top-K 检索，把命中的内容拼进 Prompt，再让模型生成。 ### 再补一句（高级）： > 我们会严格控制拼接的上下文长度，避免无关信息干扰模型。 --- ## 六、当被问：模型慢 / 不稳定怎么办？ ### 模板答案（直接背）： > 我们是把 LLM 当成一个可能失败的外部依赖来设计的。 然后说这 4 点： 1. 超时控制 2. 有条件重试 3. 降级方案 4. 异步处理 --- ## 七、当被问：你踩过什么坑？（最关键） ### 推荐你提前准备一个“真实但不致命”的坑： ### 示例答案： > 一开始 Prompt 里上下文拼得太多，导致 token 成本暴涨，而且响应时间很不稳定。后来通过： > - 限制 Top-K > - 加摘要 > - 加缓存 > 成本和稳定性才下来。 📌 **这个回答 = 真实 + 工程 + 成长** --- ## 八、面试官杀手追问，你这样接 ### Q：你怎么判断 LLM 输出是不是错的？ 👉 推荐答法： > 涉及关键业务数据时，我们不会完全相信模型输出，而是： > - 关键值只来源于系统 > - LLM 只做解释或结构化 > - 必要时有人审 这句话非常“企业级”。 --- ## 🧠 终极提醒（非常重要） ### ❌ 不要说： - “模型挺聪明的” - “基本没什么问题” - “看感觉” ### ✅ 多说： - 控制 - 约束 - 降级 - 兜底 - 成本 - 稳定性 --- ## 🚀 下一步我可以继续这样帮你： - 🧪 **模拟一整场 LLM 面试（我当面试官）** - 📐 **教你画一张 3 分钟能讲完的 LLM 架构图** - 📝 **帮你把简历里的 LLM 项目描述“工程化重写”** 你现在是 **已经有 LLM 项目经历**，还是 **准备用“学习 + Demo 项目”去面试**？ 我可以按你真实情况，直接给你定制“最安全的说法”。