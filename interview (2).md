# 企业 IT 数字化系统 / AI 工程方向面试题（含参考答案）

## 一、Python & 后端基础（必问）

### 1. 你在项目中是如何使用 Python 的？主要用在哪些场景？

**考察点**：Python 实战深度、是否偏脚本 or 服务端

**参考答案：**

```
在项目中 Python 主要用于三类场景：
1. 后端服务开发（基于 Django / FastAPI），提供 RESTful API
2. 数据处理与自动化脚本，例如：
   - 定时数据同步
   - 日志分析
   - 业务报表生成
3. 对接外部系统或第三方 API（如 LLM API、内部系统接口）

在工程实践中会注意：
- 使用虚拟环境（venv / conda）
- 模块化设计，避免脚本堆叠
- 日志、异常捕获与配置化
```

------

### 2. Python 中如何进行异常处理？项目中是如何设计异常体系的？

**考察点**：代码健壮性、工程意识

**参考答案：**

```
Python 使用 try / except / finally 进行异常处理。

在项目中通常会：
- 定义业务异常类（继承 Exception）
- 区分：
  - 参数异常
  - 外部服务异常
  - 数据库异常
- 在 API 层统一捕获异常并返回标准错误码

例如：
- HTTP 400：参数错误
- HTTP 500：系统内部错误
- HTTP 502：外部服务异常

同时配合日志系统记录完整 traceback，便于排查问题。
```

------

## 二、Web 开发 & RESTful API

### 3. 你如何理解前后端分离架构？

**考察点**：Web 基础、工程认知

**参考答案：**

```
前后端分离指：
- 前端（Vue / React）负责 UI 与交互
- 后端（Django / FastAPI）提供数据接口

优点包括：
- 前后端可独立开发、部署
- 接口标准化（REST / JSON）
- 更利于多端接入（Web / App）

后端关注：
- 接口设计
- 数据校验
- 权限控制
- 性能与安全
```

------

### 4. 设计一个 RESTful API 时你会考虑哪些方面？

**考察点**：接口规范、实战经验

**参考答案：**

```
主要考虑：
1. URL 语义清晰（资源导向）
   - GET /api/users
   - POST /api/users
2. HTTP 方法语义正确
3. 状态码规范
4. 参数校验（前后端都做）
5. 接口版本控制（如 /api/v1）
6. 返回结构统一：
   - code
   - message
   - data
```

------

## 三、数据库（PostgreSQL / MySQL）

### 5. PostgreSQL 和 MySQL 有什么区别？项目中如何选择？

**考察点**：数据库认知

**参考答案：**

```
主要区别：
- PostgreSQL：
  - 支持复杂查询、JSON、GIS
  - 更严格的事务一致性
  - 更适合复杂业务系统
- MySQL：
  - 使用广泛
  - 运维简单
  - 读性能较好

选择依据：
- 复杂数据模型、分析型查询：PostgreSQL
- 业务简单、高并发读：MySQL
```

------

### 6. 项目中如何优化数据库性能？

**考察点**：实战能力

**参考答案：**

```
常见手段：
- 合理建索引（避免过多）
- 避免 SELECT *
- 分页查询
- 慢 SQL 分析
- 读写分离（如有条件）
- 使用缓存（Redis）

同时会通过：
- EXPLAIN 分析执行计划
```

------

## 四、分布式 & 架构设计

### 7. 你如何理解分布式系统？

**考察点**：系统设计基础

**参考答案：**

```
分布式系统指多个服务节点协同完成一个业务。

核心挑战：
- 数据一致性
- 服务通信
- 容错与可用性

常见手段：
- 服务拆分
- API 网关
- 消息队列
- 服务注册与发现
```

------

### 8. 如果系统访问量突然升高，你会如何设计架构？

**考察点**：架构思维（加分）

**参考答案：**

```
思路：
1. 服务无状态化
2. 使用负载均衡（Nginx）
3. 水平扩展服务实例
4. 数据库优化 / 分库
5. 引入缓存
6. 监控与告警

优先从最容易见效的优化入手。
```

------

## 五、LLM & 机器学习工程化（重点）

### 9. 你是如何在项目中对接大模型（LLM）API 的？

**考察点**：是否真的做过

**参考答案：**

```
一般流程：
1. 封装统一的 LLM 调用模块
2. 处理：
   - Prompt 构造
   - Token 限制
   - 超时与重试
3. 对返回结果进行结构化处理
4. 结合业务逻辑落地，如：
   - 文档摘要
   - 问答
   - 数据分析辅助

会注意：
- 调用成本控制
- 日志记录
- 降级方案
```

------

### 10. PyTorch 和 TensorFlow 的主要区别？

**考察点**：机器学习基础

**参考答案：**

```
PyTorch：
- 动态计算图
- 更易调试
- 学术和工程都流行

TensorFlow：
- 静态图（2.x 改善）
- 工程化和部署生态成熟

项目中通常：
- 训练和实验用 PyTorch
- 部署和推理用 TensorFlow / ONNX
```

------

## 六、Docker / Kubernetes / 运维

### 11. Docker 在项目中是如何使用的？

**考察点**：工程化能力

**参考答案：**

```
Docker 主要用于：
- 统一运行环境
- 简化部署
- 避免环境不一致问题

常见流程：
- 编写 Dockerfile
- 构建镜像
- 通过 docker-compose 或 K8s 运行
```

------

### 12. Kubernetes 解决了什么问题？

**考察点**：K8s 理解深度

**参考答案：**

```
Kubernetes 主要解决：
- 容器编排
- 自动扩缩容
- 服务发现
- 故障自愈

适用于：
- 微服务架构
- 多环境部署
```

------

### 13. Linux 下你常用哪些命令？

**考察点**：基础但必问

**参考答案：**

```
常用命令：
- 文件：ls、cd、cp、mv、rm
- 查看日志：cat、less、tail -f
- 进程：ps、top、kill
- 网络：netstat、curl
- 权限：chmod、chown
```

------

## 七、GitLab & CI/CD

### 14. 你在 GitLab 中的开发流程是怎样的？

**考察点**：团队协作经验

**参考答案：**

```
常见流程：
1. 基于主分支创建 feature 分支
2. 提交代码并 Push
3. 创建 Merge Request
4. Code Review
5. 合并到主分支

配合 GitLab CI：
- 自动测试
- 自动构建 Docker 镜像
- 自动部署
```

------

## 八、综合场景题（非常加分）

### 15. 如果让你设计一个“内部 AI 助手系统”，你会怎么做？

**考察点**：系统设计 + LLM 落地能力

**参考答案：**

```
整体架构：
- 前端：Vue / React
- 后端：Python（Django / FastAPI）
- LLM 服务层：
  - Prompt 模板
  - 调用封装
- 数据层：
  - PostgreSQL
- 部署：
  - Docker + Kubernetes

功能：
- 内部知识问答
- 自动生成报表
- 业务流程辅助

重点关注：
- 权限控制
- 数据安全
- 成本与性能
```

# 一、如何评估 **LLM 的能力边界**

> 核心目标：**这个模型“天生能做到什么”，以及“做不到什么”**

### 1️⃣ 能力边界的 5 个维度（业界共识）

| 维度        | 要评什么           | 常见误判             |
| ----------- | ------------------ | -------------------- |
| 知识边界    | 预训练里有没有     | 把知识缺失当成推理差 |
| 推理深度    | 多步、反事实、约束 | 用简单 QA 高估模型   |
| 指令遵循    | 复杂约束是否遵守   | Prompt 写得好≠模型强 |
| 稳定性      | 同题多次回答一致性 | 单次回答误判         |
| 安全 & 拒答 | 会不会乱编         | 把幻觉当“创造力”     |

------

### 2️⃣ 评估方法（**不用 benchmark 也能做**）

#### ✅（1）能力探针法（Capability Probing）

人为设计「**卡点问题**」：

```
问题必须满足：
- 无法通过搜索直接答
- 需要 ≥3 步推理
- 含隐性约束
```

例子：

```
一个会议持续90分钟，中途每25分钟休息5分钟，
实际讨论时间是多少？
```

👉 用来测试 **推理 + 数学 + 约束理解**

------

#### ✅（2）同义扰动测试（Robustness）

同一个问题，轻微改写 5–10 次：

```
请总结以下内容（正式）
请用三点说明
请只用一句话总结
```

📌 看的是：
 **模型是否“理解任务本质”，还是“记住 prompt 模板”**

------

#### ✅（3）一致性测试（Self-consistency）

- 同一问题跑 N 次（temperature > 0）
- 看答案是否**收敛**

📌 不收敛 ⇒ 能力边界附近

------

### 3️⃣ 能力边界输出建议

你最终应该得到类似这样的结论：

```
该模型：
- 2步以内逻辑推理稳定
- 数值推理在约束叠加时明显退化
- 长指令（>400 tokens）遵循率下降
- 无外部知识时幻觉风险高
```

👉 **这是后面 RAG / 微调的决策基础**

------

# 二、如何评估 **RAG 系统**

> 核心目标：
>  **问题是没被“找到”，还是被“找到了但没用好”？**

### 1️⃣ RAG 必须拆成 3 层评估（非常重要）

```
检索（Retrieval）
→ 证据利用（Grounding）
→ 回答生成（Generation）
```

90% 的 RAG 失败，**不是模型问题**

------

### 2️⃣ 检索层评估（最关键）

#### 📊 核心指标

| 指标     | 含义               |
| -------- | ------------------ |
| Recall@K | 正确文档是否被召回 |
| MRR      | 排名是否靠前       |
| 覆盖率   | 答案是否存在于文档 |

#### 实操方法（推荐）

- 人工构造 **Q → 标准证据文档**
- 统计：

```
问题100个
→ 文档被检索到 82 次
→ Recall@5 = 0.82
```

📌 Recall < 0.7 → **别谈生成**

------

### 3️⃣ 证据利用评估（RAG 的“灵魂”）

#### 关键问题：

- 模型是否真的 **用到了文档**
- 还是“看了但没用 / 编造”

#### 方法一：引用一致性

```
回答中的结论
是否能在文档中逐句对齐
```

#### 方法二：证据剥离测试（🔥 很狠）

- 把关键证据删掉
- 再问一次

👉 如果答案**没明显变差**
 → 模型其实没用 RAG

------

### 4️⃣ 生成层评估

| 指标                | 看什么       |
| ------------------- | ------------ |
| Faithfulness        | 是否忠于文档 |
| Hallucination rate  | 是否乱编     |
| Answer completeness | 是否答全     |

📌 很多团队只看 BLEU / ROUGE
 👉 **这是错误的（对 RAG 几乎无效）**

------

# 三、如何评估 **微调（Fine-tuning）效果**

> 核心目标：
>  **微调是“增强了能力”，还是“破坏了泛化”？**

------

### 1️⃣ 微调评估的 3 个对照组（必须）

```
Base 模型
vs
Base + Prompt
vs
Fine-tuned 模型
```

❌ 只对比「微调前 vs 微调后」是**不合格评估**

------

### 2️⃣ 微调真正该提升的是什么？

| 微调类型 | 正确目标                   |
| -------- | -------------------------- |
| SFT      | 输出风格 / 格式 / 领域表达 |
| LoRA     | 指令稳定性 / 任务专精      |
| 全参     | 极少用（成本 & 风险）      |

📌 微调**不该**显著提升：

- 推理深度
- 数学能力
- 常识知识

（这些是预训练决定的）

------

### 3️⃣ 核心评估维度

#### ✅（1）任务内效果

- 准确率 / Pass@1
- 输出格式合规率
- 人工评分（必不可少）

#### ✅（2）泛化退化测试（⚠️ 很重要）

用**非训练分布**问题测试：

```
训练：客服问答
测试：相似但未出现的投诉场景
```

👉 微调后反而更差 = 过拟合

------

#### ✅（3）灾难性遗忘

问一些 **通用问题**：

```
基础数学
通用推理
常识问答
```

📌 明显下降 ⇒ 微调“伤模型”

------

### 4️⃣ 一个健康的微调结果应该是：

```
- 领域任务 ↑↑
- 指令遵循 ↑
- 通用能力 ↓ 不明显
- 推理能力 ≈ 不变
```

------

# 四、三者的**联合评估与决策逻辑（非常关键）**

### 正确的工程决策路径是：

```
1️⃣ 先评估 Base LLM 能力边界
2️⃣ 如果是“知识缺失” → RAG
3️⃣ 如果是“表达 / 稳定性” → 微调
4️⃣ RAG + 微调 要分开评估
```

------

### ❌ 常见错误决策

| 错误               | 后果          |
| ------------------ | ------------- |
| 用微调补知识       | 幻觉 + 过拟合 |
| RAG 召回差却调模型 | 无解          |
| 微调后不测泛化     | 上线翻车      |

------

# 五、给你一套**一句话判断法**

> 🔥 **非常实用**

- **回答错误，但证据没被检索到** → RAG 问题
- **证据在，但回答没用** → Prompt / 对齐问题
- **格式乱、风格不稳** → 微调
- **逻辑本身错** → 模型能力边界

------

如果你愿意，我可以下一步帮你：

- ✅ 给你一个 **企业级评估 checklist**
- ✅ 设计 **自动化评估脚本（RAG + 微调）**
- ✅ 结合你现在用的模型（Qwen / LLaMA / Claude / Azure OpenAI）给**针对性指标**

你现在更偏 **研究评估**，还是 **线上系统落地评估**？

✅ 给你一个 企业级评估 checklist ✅ 设计 自动化评估脚本（RAG + 微调） ✅ 结合你现在用的模型（ Azure OpenAI）给针对性指标

太好了，这三个点**正好是一套企业级“评估中台”该有的配置**。
 我直接给你一份**可落地、能交付、能进评审会的方案**👇
 （我会默认你是 **Azure OpenAI + 企业内部 RAG + 可能有 SFT/LoRA**）

------

# 一、企业级 LLM / RAG / 微调 **统一评估 Checklist（可直接用）**

> 👉 这份 checklist 的目标是：
>  **“任何人改模型、改 Prompt、改向量库，都必须跑一遍”**

------

## ✅ A. Base LLM 能力边界评估（Azure OpenAI）

### A1. 基础能力（一次性 / 大版本升级时）

-  多步推理（≥3 step）通过率
-  数值 / 逻辑混合推理
-  长指令遵循（≥500 tokens）
-  JSON / Schema 严格输出
-  多轮上下文一致性
-  Temperature 扰动稳定性

📌 **指标建议**

- Pass@1
- Self-consistency（5 次采样一致率）
- Schema 合规率 %

------

### A2. 安全 & 幻觉基线

-  无资料时是否明确拒答
-  编造来源检测
-  不确定性表达能力（“不知道”）

📌 **输出结论模板**

```
模型 X 在无外部知识下：
- 幻觉率：12%
- 拒答准确率：91%
```

------

## ✅ B. RAG 系统评估 Checklist（上线前必过）

### B1. Retrieval（检索层）【不达标直接打回】

-  Recall@5 ≥ 0.8
-  Recall@10 ≥ 0.9
-  MRR ≥ 0.6
-  文档切分粒度合理（答案不跨 chunk）

📌 Azure 常见坑：

- embedding 模型不统一
- chunk 太大（>1k tokens）

------

### B2. Grounding（证据利用）

-  回答是否能逐条溯源
-  删除关键证据后答案明显变差
-  引用是否准确（页码 / 段落）

📌 **这是 RAG 的“生死线”**

------

### B3. Generation（生成层）

-  忠实于文档（Faithfulness）
-  覆盖问题所有子问
-  无外延编造

------

## ✅ C. 微调（SFT / LoRA）评估 Checklist

### C1. 对照组（必须）

-  Base 模型
-  Base + Prompt
-  Fine-tuned 模型

------

### C2. 任务内效果

-  准确率 / 通过率
-  输出格式合规率
-  人工评分（≥2 人）

------

### C3. 泛化 & 遗忘

-  OOD 测试集性能
-  通用推理能力对比
-  幻觉率变化

📌 **任何一项明显下降 = 微调失败**

------

# 二、自动化评估脚本设计（企业级）

下面是**真实可跑的结构设计**（不是 demo）

------

## 🧱 1️⃣ 评估数据结构（统一）

```
{
  "id": "rag_023",
  "question": "合同的提前解约条款是什么？",
  "gold_answer": "需提前30天书面通知...",
  "gold_docs": ["doc_12", "doc_45"],
  "eval_type": ["retrieval", "grounding", "generation"]
}
```

------

## 🔍 2️⃣ RAG 自动评估脚本（核心）

### 2.1 检索评估

```
def eval_retrieval(retrieved_docs, gold_docs, k=5):
    hit = any(doc in retrieved_docs[:k] for doc in gold_docs)
    recall = hit / len(gold_docs)
    return recall
```

📌 企业建议输出：

- Recall@5
- Recall@10
- 未命中问题列表（给知识团队）

------

### 2.2 Grounding 评估（LLM-as-a-Judge）

```
judge_prompt = f"""
给定回答和参考文档：
1. 回答是否完全基于文档？
2. 是否有文档外推断？
只输出 JSON：
{{"faithful": true/false, "reason": "..."}}
"""
```

📌 Azure GPT-4o / GPT-4.1 非常适合做 Judge
 （**不要用同一个模型做生成 + 评估**）

------

### 2.3 证据剥离测试（强烈推荐）

```
answer_full = rag_answer(question, docs)
answer_removed = rag_answer(question, docs_without_key)

score_diff = judge(answer_full) - judge(answer_removed)
```

👉 `score_diff < threshold` ⇒ **RAG 没生效**

------

## 🧪 3️⃣ 微调自动评估脚本

### 3.1 任务准确率

```
def exact_match(pred, gold):
    return pred.strip() == gold.strip()
```

### 3.2 格式合规率（企业必备）

```
import json

def schema_valid(output):
    try:
        json.loads(output)
        return True
    except:
        return False
```

------

### 3.3 灾难性遗忘检测

```
baseline_score = eval_general_tasks(base_model)
ft_score = eval_general_tasks(ft_model)

delta = ft_score - baseline_score
```

📌 `delta < -5%` → **红线**

------

# 三、Azure OpenAI **针对性评估指标 & 建议**

你这点非常关键 👍

------

## 🔷 1️⃣ Azure 模型组合推荐（评估用）

| 用途      | 推荐模型               |
| --------- | ---------------------- |
| 生成      | GPT-4o / GPT-4.1       |
| Judge     | GPT-4.1（低 temp）     |
| Embedding | text-embedding-3-large |
| 成本测试  | gpt-4o-mini            |

------

## 🔷 2️⃣ Azure 特有评估点（很多人忽略）

### ✅ Content Filter 影响评估

- 是否误伤业务问题
- 是否截断回答

📌 **评估时必须记录**

```
finish_reason
content_filter_results
```

------

### ✅ Latency & TPS（企业必须）

| 指标            | 建议 |
| --------------- | ---- |
| P95 latency     | < 2s |
| RAG 总延迟      | < 4s |
| Token / request | 稳定 |

------

### ✅ Prompt 版本评估（非常重要）

```
Prompt v1 vs v2
→ 相同测试集
→ 自动对比
```

📌 不评估 Prompt = 黑箱调参

------

## 🔷 3️⃣ Azure RAG 常见失败模式（你可以直接拿去讲）

| 现象             | 根因                    |
| ---------------- | ----------------------- |
| 回答像“没看文档” | Recall 低               |
| 回答正确但无引用 | Prompt 对齐差           |
| 偶发胡编         | Chunk 跨界              |
| 延迟暴涨         | embedding / rerank 串行 |

------

# 四、最终你应该输出的 **企业级评估报告结构**

```
1. Base LLM 能力边界
2. RAG 检索 & Grounding 指标
3. 微调收益 vs 风险
4. Prompt / 模型 / 数据对照
5. 是否可上线（Go / No-Go）
```

------

## 🔥 如果你愿意，我可以继续帮你：

- 📊 **直接给你一份 Excel / CSV 评估模板**
- 🧠 帮你设计 **“评估即 CI” 的流水线**
- 🏢 按你真实业务（客服 / 法务 / 内部知识）定制测试集
- 🧪 给你一个 **RAG Debug Playbook**